import scipy.stats
import random
import math
import numpy as np
import matplotlib.pyplot as plt


def gainBinaire(machine, n):
    p = machine[n]
    return int(scipy.stats.bernoulli.rvs(p, size=1))


def aleatoire(gains, nbres):
    return random.randint(0, len(gains)-1)


def greedy(gains, nbres):
    return np.argmax(gains)


def Egreedy(gains, nbres):
    E = 0.1
    if (int(scipy.stats.bernoulli.rvs(E, size=1)) == 1):
        return aleatoire(gains, nbres)
    else:
        return greedy(gains, nbres)


def ucb(gains, nbres):
    g = np.array(gains)
    n = np.array(nbres)
    T = sum(nbres) + 1
    result = 2*math.log(T)
    result = result / n
    result = np.sqrt(result)
    result = g + result
    return np.argmax(result)
###############################################################################################################################
# Simuler les differents algorithmes
# simulation(t, l, s, e=0)
# l : nombre de leviers
# t : nombre de tentatives
# s : strategie 1 : aleatoire , 2 : greedy, 3 : Egreedy , 4 : ucb
# e : nombre de tentatives en exploration uniforme


def simulation(t, l, s, e=0):
    # parametres des lois bernoullis reels
    random.random()
    gains_effectis = [random.random() for x in range(l)]
    random.shuffle(gains_effectis)
    # liste des recompenses moyennes estimees pour chaque levier
    gains = [0]*l
    # liste des nombres de fois ou chaque levier a ete actionne
    nbres = [0]*l
    # phase d'exploration uniforme
    for i in range(e):
        # determination du levier a actionner
        action = aleatoire(gains, nbres)
        # determination du gain
        gain = gainBinaire(gains_effectis, action)
        # mise a jour de l'information connue (gains et nbres)
        gains[action] = (gains[action]*nbres[action]+gain) / (nbres[action]+1)
        nbres[action] += 1
    # fin de la phase d'exploration

    gains_joueur = []
    gains_ideal = []
    regret = []
    temps = []
    max = 0
    real = 0

    print("les parametres reels :\n", gains_effectis)
    for i in range(t):
        print("tentative numero :", i)
        print("liste des recompenses moyennes estimees pour chaque levier :\n", gains)
        print("liste des nombres de fois ou chaque levier a ete actionne :\n", nbres)
        # determination du levier a actionner
        if (s == 1):
            action = aleatoire(gains, nbres)
            print("action aleatoire :", action)
            label = "algorithme aleatoire"
        elif (s == 2):
            action = greedy(gains, nbres)
            print("action greedy :", action)
            label = "algorithme greedy"
        elif (s == 3):
            action = Egreedy(gains, nbres)
            print("action Egreedy :", action)
            label = "algorithme Egreedy"
        elif (s == 4):
            action = ucb(gains, nbres)
            print("action ucb :", action)
            label = "algorithme ucb"
        else:
            print("erreur : aucune strategie ne porte ce numero")
        # determination du gain
        gain = gainBinaire(gains_effectis, action)
        print("le gain est de :", gain)
        # mise a jour de l'information connue (gains et nbres)
        gains[action] = (gains[action]*nbres[action]+gain) / (nbres[action]+1)
        nbres[action] += 1
        # mise a jour du gain du joueur
        gains_joueur.append(gain)
        # determination de la somme des recompenses obtenue jusqu'a l'instant t
        real += gain
        # mise a jour du gain maximale
        maxPos = np.argmax(gains_effectis)
        gain_max = gainBinaire(gains_effectis, maxPos)
        gains_ideal.append(gain_max)
        print("le meilleur gain :", gain_max)
        # determination de la somme des recompenses maximale
        max += gain_max
        # mise a jour du regret
        regret.append(max - real)
        print("la somme de vos gains :", real)
        print("la somme des gains dans le meilleur des cas :", max)
        print("le regret est :", max - real)
        temps.append(i)
    print("les parametres reels :\n", gains_effectis)
    gains_joueur = np.cumsum(gains_joueur)
    gains_ideal = np.cumsum(gains_ideal)
    #regret = np.cumsum(regret)
    p1, = plt.plot(temps, gains_joueur)
    p2, = plt.plot(temps, regret)
    p3, = plt.plot(temps, gains_ideal)
    plt.xlabel("nombre de tentatives")
    plt.ylabel("gain du joueur - gain ideal - regret")
    plt.title(label)
    plt.legend([p1, p2, p3], ["gain du joueur", "le regret", "gain max"])
    label2 = "nombre de leviers : " + str(l)
    label3 = "nombre total de tentatives : " + str(t)
    label4 = "nombre de tentative en exploration : " + str(e)
    plt.text(0, 4500, label2)
    plt.text(0, 5000, label3)
    plt.text(0, 5500, label4)
    plt.show()
##############################################################################################################################
# Comparer le regret des different algorithmes en fonction du nombre de parties
#comparaison_nombre_de_parties(t, l, e=0)
# t : nombre total de tentatives
# l : nombre de leviers
# e : nombre de tentatives en exploration


def comparaison_nombre_de_parties(t, l, e=0):
    # parametres des lois bernoullis reels
    random.random()
    gains_effectis = [random.random() for x in range(l)]
    random.shuffle(gains_effectis)
    # liste des recompenses moyennes estimees pour chaque levier
    gains_aleatoire = [0]*l
    gains_greedy = [0]*l
    gains_Egreedy = [0]*l
    gains_ucb = [0]*l
    # liste des nombres de fois ou chaque levier a ete actionne
    nbres_aleatoire = [0]*l
    nbres_greedy = [0]*l
    nbres_Egreedy = [0]*l
    nbres_ucb = [0]*l
    # phase d'exploration uniforme
    for i in range(e):
        # determination du levier a actionner
        action_aleatoire = aleatoire(gains_aleatoire, nbres_aleatoire)
        action_greedy = aleatoire(gains_greedy, nbres_greedy)
        action_Egreedy = aleatoire(gains_Egreedy, nbres_Egreedy)
        action_ucb = aleatoire(gains_ucb, nbres_ucb)
        # determination du gain
        gain_aleatoire = gainBinaire(gains_effectis, action_aleatoire)
        gain_greedy = gainBinaire(gains_effectis, action_greedy)
        gain_Egreedy = gainBinaire(gains_effectis, action_Egreedy)
        gain_ucb = gainBinaire(gains_effectis, action_ucb)
        # mise a jour de l'information connue (gains et nbres)
        gains_aleatoire[action_aleatoire] = (
            gains_aleatoire[action_aleatoire]*nbres_aleatoire[action_aleatoire]+gain_aleatoire) / (nbres_aleatoire[action_aleatoire]+1)
        nbres_aleatoire[action_aleatoire] += 1

        gains_greedy[action_greedy] = (
            gains_greedy[action_greedy]*nbres_greedy[action_greedy]+gain_greedy) / (nbres_greedy[action_greedy]+1)
        nbres_greedy[action_greedy] += 1

        gains_Egreedy[action_Egreedy] = (
            gains_Egreedy[action_Egreedy]*nbres_Egreedy[action_Egreedy]+gain_Egreedy) / (nbres_Egreedy[action_Egreedy]+1)
        nbres_Egreedy[action_Egreedy] += 1

        gains_ucb[action_ucb] = (
            gains_ucb[action_ucb]*nbres_ucb[action_ucb]+gain_ucb) / (nbres_ucb[action_ucb]+1)
        nbres_aleatoire[action_ucb] += 1
    # fin de la phase d'exploration

    gains_joueur_aleatoire = []
    gains_joueur_greedy = []
    gains_joueur_Egreedy = []
    gains_joueur_ucb = []
    gains_ideal = []
    regret_aleatoire = []
    regret_greedy = []
    regret_Egreedy = []
    regret_ucb = []
    temps = []
    max = 0
    real_aleatoire = 0
    real_greedy = 0
    real_Egreedy = 0
    real_ucb = 0

    print("les parametres reels :\n", gains_effectis)
    for i in range(t):
        print("tentative numero :", i)
        # determination du levier a actionner
        action_aleatoire = aleatoire(gains_aleatoire, nbres_aleatoire)
        action_greedy = greedy(gains_greedy, nbres_greedy)
        action_Egreedy = Egreedy(gains_Egreedy, nbres_Egreedy)
        action_ucb = ucb(gains_ucb, nbres_ucb)
        # determination du gain
        gain_aleatoire = gainBinaire(gains_effectis, action_aleatoire)
        gain_greedy = gainBinaire(gains_effectis, action_greedy)
        gain_Egreedy = gainBinaire(gains_effectis, action_Egreedy)
        gain_ucb = gainBinaire(gains_effectis, action_ucb)
        # mise a jour de l'information connue (gains et nbres)
        gains_aleatoire[action_aleatoire] = (
            gains_aleatoire[action_aleatoire]*nbres_aleatoire[action_aleatoire]+gain_aleatoire) / (nbres_aleatoire[action_aleatoire]+1)
        nbres_aleatoire[action_aleatoire] += 1

        gains_greedy[action_greedy] = (
            gains_greedy[action_greedy]*nbres_greedy[action_greedy]+gain_greedy) / (nbres_greedy[action_greedy]+1)
        nbres_greedy[action_greedy] += 1

        gains_Egreedy[action_Egreedy] = (
            gains_Egreedy[action_Egreedy]*nbres_Egreedy[action_Egreedy]+gain_Egreedy) / (nbres_Egreedy[action_Egreedy]+1)
        nbres_Egreedy[action_Egreedy] += 1

        gains_ucb[action_ucb] = (
            gains_ucb[action_ucb]*nbres_ucb[action_ucb]+gain_ucb) / (nbres_ucb[action_ucb]+1)
        nbres_ucb[action_ucb] += 1
        # mise a jour du gain du joueur
        gains_joueur_aleatoire.append(gain_aleatoire)
        gains_joueur_greedy.append(gain_greedy)
        gains_joueur_Egreedy.append(gain_Egreedy)
        gains_joueur_ucb.append(gain_ucb)
        # determination de la somme des recompenses obtenue jusqu'a l'instant t
        real_aleatoire += gain_aleatoire
        real_greedy += gain_greedy
        real_Egreedy += gain_Egreedy
        real_ucb += gain_ucb
        # mise a jour du gain maximale
        maxPos = np.argmax(gains_effectis)
        gain_max = gainBinaire(gains_effectis, maxPos)
        gains_ideal.append(gain_max)
        # determination de la somme des recompenses maximale
        max += gain_max
        # mise a jour du regret
        regret_aleatoire.append(max - real_aleatoire)
        regret_greedy.append(max - real_greedy)
        regret_Egreedy.append(max - real_Egreedy)
        regret_ucb.append(max - real_ucb)
        temps.append(i)
    #regret = np.cumsum(regret)
    p1, = plt.plot(temps, regret_aleatoire)
    p2, = plt.plot(temps, regret_greedy)
    p3, = plt.plot(temps, regret_Egreedy)
    p4, = plt.plot(temps, regret_ucb)
    plt.xlabel("nombre de tentatives")
    plt.ylabel("regret")
    plt.title(
        "Comparaison sur l'evolution du regret en fonction du nombre de tentatives")
    plt.legend([p1, p2, p3, p4], ["aleatoire", "greedy", "Egreedy", "ucb"])
    label2 = "nombre de leviers : " + str(l)
    label3 = "nombre total de tentatives : " + str(t)
    label4 = "nombre de tentative en exploration : " + str(e)
    plt.text(0, 2000, label2)
    plt.text(0, 2500, label3)
    plt.text(0, 3000, label4)
    plt.show()

#########################################################################################################################################
# Comparaison du regret des differents algorithmes en fonction du nombre de leviers
# comparaison_nombre_de_leviers(inf, sup, t, e=0)
# inf et sup : bornes a donner pour le nombre de leviers
# t : nombre de tentatives a tester pour chaque combinaison de leviers
# e : nombre de tentatives en exploration pour chaque combinaison de leviers


def comparaison_nombre_de_leviers(inf, sup, t, e=0):
    leviers = []
    reg1 = []
    reg2 = []
    reg3 = []
    reg4 = []
    # on fait varier le nombre de leviers
    for i in range(inf, sup):
        print("Nombre de leviers :", i)
        leviers.append(i)
        # parametres des lois bernoullis reels
        random.random()
        gains_effectis = [random.random() for x in range(i)]
        random.shuffle(gains_effectis)
        # liste des recompenses moyennes estimees pour chaque levier
        gains_aleatoire = [0]*i
        gains_greedy = [0]*i
        gains_Egreedy = [0]*i
        gains_ucb = [0]*i
        # liste des nombres de fois ou chaque levier a ete actionne
        nbres_aleatoire = [0]*i
        nbres_greedy = [0]*i
        nbres_Egreedy = [0]*i
        nbres_ucb = [0]*i
        # phase d'exploration uniforme
        for k in range(e):
            # determination du levier a actionner
            action_aleatoire = aleatoire(gains_aleatoire, nbres_aleatoire)
            action_greedy = aleatoire(gains_greedy, nbres_greedy)
            action_Egreedy = aleatoire(gains_Egreedy, nbres_Egreedy)
            action_ucb = aleatoire(gains_ucb, nbres_ucb)
            # determination du gain
            gain_aleatoire = gainBinaire(gains_effectis, action_aleatoire)
            gain_greedy = gainBinaire(gains_effectis, action_greedy)
            gain_Egreedy = gainBinaire(gains_effectis, action_Egreedy)
            gain_ucb = gainBinaire(gains_effectis, action_ucb)
            # mise a jour de l'information connue (gains et nbres)
            gains_aleatoire[action_aleatoire] = (
                gains_aleatoire[action_aleatoire]*nbres_aleatoire[action_aleatoire]+gain_aleatoire) / (nbres_aleatoire[action_aleatoire]+1)
            nbres_aleatoire[action_aleatoire] += 1

            gains_greedy[action_greedy] = (
                gains_greedy[action_greedy]*nbres_greedy[action_greedy]+gain_greedy) / (nbres_greedy[action_greedy]+1)
            nbres_greedy[action_greedy] += 1

            gains_Egreedy[action_Egreedy] = (
                gains_Egreedy[action_Egreedy]*nbres_Egreedy[action_Egreedy]+gain_Egreedy) / (nbres_Egreedy[action_Egreedy]+1)
            nbres_Egreedy[action_Egreedy] += 1

            gains_ucb[action_ucb] = (
                gains_ucb[action_ucb]*nbres_ucb[action_ucb]+gain_ucb) / (nbres_ucb[action_ucb]+1)
            nbres_aleatoire[action_ucb] += 1
        # fin de la phase d'exploration

        gains_joueur_aleatoire = []
        gains_joueur_greedy = []
        gains_joueur_Egreedy = []
        gains_joueur_ucb = []
        gains_ideal = []
        regret_aleatoire = []
        regret_greedy = []
        regret_Egreedy = []
        regret_ucb = []
        max = 0
        real_aleatoire = 0
        real_greedy = 0
        real_Egreedy = 0
        real_ucb = 0
        for o in range(t):
            # determination du levier a actionner
            action_aleatoire = aleatoire(gains_aleatoire, nbres_aleatoire)
            action_greedy = greedy(gains_greedy, nbres_greedy)
            action_Egreedy = Egreedy(gains_Egreedy, nbres_Egreedy)
            action_ucb = ucb(gains_ucb, nbres_ucb)
            # determination du gain
            gain_aleatoire = gainBinaire(gains_effectis, action_aleatoire)
            gain_greedy = gainBinaire(gains_effectis, action_greedy)
            gain_Egreedy = gainBinaire(gains_effectis, action_Egreedy)
            gain_ucb = gainBinaire(gains_effectis, action_ucb)
            # mise a jour de l'information connue (gains et nbres)
            gains_aleatoire[action_aleatoire] = (
                gains_aleatoire[action_aleatoire]*nbres_aleatoire[action_aleatoire]+gain_aleatoire) / (nbres_aleatoire[action_aleatoire]+1)
            nbres_aleatoire[action_aleatoire] += 1

            gains_greedy[action_greedy] = (
                gains_greedy[action_greedy]*nbres_greedy[action_greedy]+gain_greedy) / (nbres_greedy[action_greedy]+1)
            nbres_greedy[action_greedy] += 1

            gains_Egreedy[action_Egreedy] = (
                gains_Egreedy[action_Egreedy]*nbres_Egreedy[action_Egreedy]+gain_Egreedy) / (nbres_Egreedy[action_Egreedy]+1)
            nbres_Egreedy[action_Egreedy] += 1

            gains_ucb[action_ucb] = (
                gains_ucb[action_ucb]*nbres_ucb[action_ucb]+gain_ucb) / (nbres_ucb[action_ucb]+1)
            nbres_ucb[action_ucb] += 1
            # mise a jour du gain du joueur
            gains_joueur_aleatoire.append(gain_aleatoire)
            gains_joueur_greedy.append(gain_greedy)
            gains_joueur_Egreedy.append(gain_Egreedy)
            gains_joueur_ucb.append(gain_ucb)
            # determination de la somme des recompenses obtenue jusqu'a l'instant t
            real_aleatoire += gain_aleatoire
            real_greedy += gain_greedy
            real_Egreedy += gain_Egreedy
            real_ucb += gain_ucb
            # mise a jour du gain maximale
            maxPos = np.argmax(gains_effectis)
            gain_max = gainBinaire(gains_effectis, maxPos)
            gains_ideal.append(gain_max)
            # determination de la somme des recompenses maximale
            max += gain_max
        reg1.append(max - real_aleatoire)
        reg2.append(max - real_greedy)
        reg3.append(max - real_Egreedy)
        reg4.append(max - real_ucb)
    #regret = np.cumsum(regret)
    # dessin des courbes
    p1, = plt.plot(leviers, reg1)
    p2, = plt.plot(leviers, reg2)
    p3, = plt.plot(leviers, reg3)
    p4, = plt.plot(leviers, reg4)
    plt.xlabel("nombre de leviers")
    plt.ylabel("regret")
    plt.title(
        "Comparaison sur l'evolution du regret en fonction du nombre de leviers")
    plt.legend([p1, p2, p3, p4], ["aleatoire", "greedy", "Egreedy", "ucb"])
    label2 = "nombre de tentatives : " + str(t)
    label3 = ""
    label4 = "nombre de tentative en exploration : " + str(e)
    plt.text(0, 500, label2)
    plt.text(0, 550, label3)
    plt.text(0, 600, label4)
    plt.show()
